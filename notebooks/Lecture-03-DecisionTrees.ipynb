{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6651e1e-8786-4771-830e-b70f5da94464",
   "metadata": {},
   "source": [
    "# Lecture 3: Decison Trees\n",
    "\n",
    "\n",
    "Applied Machine Learning \n",
    "\n",
    "Professor: __Ramesh Natarajan__ \\\n",
    "Email: **ramesh.natarajan@yu.edu** \\\n",
    "Department of Computer Science \\\n",
    "Stern College and Yeshiva University\n",
    "\n",
    "Date: Jan 29, 2024\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c166965d-3d64-479d-acb4-c58d72281da6",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "1. The starting point for developing the notebook-based instruction material was the set of notebooks provided by the previous  course instructor__[Prof. Zach Glassman](https://www.linkedin.com/in/zachary-glassman-7088844b/)__.\n",
    "2. These notebooks have been augmented by similar instruction material taken from many, many other sources, including official documentation from many ML libraries and frameworks. I have adapted and modified the content where needed to make it compatible with student level, orientation and proficiency in this course.\n",
    "3. The primary addition sources include:\n",
    "   1. Course content from V. Kuleshov, Cornell Tech __[repo](https://github.com/kuleshov/cornell-cs5785-2020-applied-ml/blob/main/notebooks/lecture1-introduction.ipynb)__.\n",
    "   2. Book and support material from Hal Daume II, __[A course in Machine Learning](http://ciml.info)__. __[repo](https://github.com/hal3/ciml/)__\n",
    "   3. Book and support material from A. Geron, __[Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow (3rd edition)](https://homl.info/er3)__. __[repo](https://github.com/ageron/handson-ml3/blob/main/README.md)__\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58540dab-73dc-4459-bc86-3079bb6112db",
   "metadata": {},
   "source": [
    "# Announcements\n",
    "\n",
    "1. Problem Set 1 due before next class  \n",
    "2. Recitations will continue review __[differential calculus](https://github.com/ageron/handson-ml2/blob/master/math_differential_calculus.ipynb)__ and __[linear algebra](https://github.com/ageron/handson-ml2/blob/master/math_linear_algebra.ipynb)__.   These 2 topics are critical for the next few lectures.\n",
    "3. Please continue  to review and refresh yourself on the mathematical prerequisites for applied machine learning from part 1 for Deisenroth, Faisal and Ong, __[Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf)__.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb7e2f-25c9-4224-82af-f8e28773cbc8",
   "metadata": {},
   "source": [
    "# What are Decision Trees?\n",
    "\n",
    "\n",
    "<table>\n",
    "    <td width=\"40%\"><center>\n",
    "        <img src=\"./img/Lecture-03/decision-tree-creative.png\"/>    \n",
    "    </center>\n",
    "    Generated by Chat-GPT \n",
    "     </td>  \n",
    "    <td>\n",
    "<ul>\n",
    "<font size = \"+1.5\">\n",
    "<li> Definition: Supervised learning models that resemble flowcharts. </li>\n",
    "<li> Use a series of questions to classify data points or predict values.</li>\n",
    "<li>Example: A decision tree classifying fruits might ask questions about color, size, and texture to decide if it's an apple, orange, or banana.</li>\n",
    "</font>\n",
    "</ul>\n",
    "\n",
    "</td></table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c884233f-ec89-480b-9d75-166af605f40d",
   "metadata": {},
   "source": [
    "# Decision Tree Algorithm\n",
    "\n",
    "+ Input features $(x_1, x_2, \\ldots, x_d) \\in X$ may be continuous or categorical\n",
    "+ Target feature $y \\in Y$  (may be continuous, binary categorical, or multiclass categorical)\n",
    "+ The feature space is recursively partitioned using splitting rules: \n",
    "    * Start will the root node consisting of the entire feature spaces\n",
    "    * Loop over all features in turn:\n",
    "        - Find the best split for each feature\n",
    "        - Compare with the overall best split and replace if better\n",
    "    * The overall best split over all features is chosen as the splitting rule\n",
    "    * For each of the nodes obtained from the split continue the splitting until no further improvement is possible - this is then a leaf node\n",
    "\n",
    "+ Use a simple prediction for the leaf node (ex. Average for continuous target, majority class for categorical target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a15c3-8be6-441c-8b17-eb789616eaa7",
   "metadata": {},
   "source": [
    "# Splitting Rules\n",
    "\n",
    "A splitting rule is one that splits the space $X$ of input features into two disjoint regions. \n",
    "\n",
    "Typically these splits are \"axis parallel\" and the splits only consider rules that involve a single feature.\n",
    "\n",
    "+ If the feature $x_j$ is continuous then split is  $(x_j \\leq t,  x_j > t)$ where $t$ is some threshold value.\n",
    "+ If the feature $x_j$ is categorical taking values in the set $S =( s_1, s_2, \\ldots s_k)$, the split is $(x_j \\in \\hat{S}, x_j \\notin \\hat{S})$ where $\\hat{S}$ is some proper subset of $S$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194b106-03c5-4bb0-beb3-51eec586cee6",
   "metadata": {},
   "source": [
    "# Splitting Criteria for Classification\n",
    "\n",
    "<center>\n",
    "<img src=\"./img/Lecture-03/tree-split.png\", width = \"250\"/>    \n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "Assume that the node $\\cal{R}$ has $N_{\\cal{R}}$ examples from $K$ classes denoted $c_1, c_2, \\ldots, c_K$.\n",
    "\n",
    "Define $p_j = {1 \\over N_{\\cal{R}}} \\sum_i^{N_{\\cal{R}}} I(y_i = c_j)$ \n",
    "\n",
    "Class prediction for Node $\\cal{R}$: $c_k$ such that $k = \\arg\\max_j p_j$.\n",
    "\n",
    "Let $\\cal{I}_{\\cal{R}}$ define the impurity at node $\\cal{R}$: \n",
    "\n",
    "1. Misclassification Error: $\\cal{I}_{\\cal{R}} = {1 \\over N_{\\cal{R}}} \\sum_i^{N_{\\cal{R}}} I(y_i \\neq c_k)$ \n",
    "\n",
    "2. Gini: $\\cal{I}_{\\cal{R}} = \\sum_{j=1}^K p_j (1-p_j)$\n",
    "\n",
    "3. Entropy: $\\cal{I}_{\\cal{R}} = -\\sum_{j=1}^K p_j \\log{p_j}$\n",
    "\n",
    "\n",
    "Assume that $\\cal{R}$ is split into 2 nodes $\\cal{S}$ and $\\cal{T}$ with examples $N_{\\cal{S}}$ and $N_{\\cal{T}}$ respectively\n",
    "\n",
    "Then the impurity loss from the split of $\\cal{R}$ into $\\cal{S}$ and $\\cal{T}$is is given by \n",
    "\n",
    "$ \\Delta \\cal{I} = \\cal{I}_{\\cal{R}} - [ {N_{\\cal{S}} \\over N_{\\cal{R}}} \\cal{I}_{\\cal{S}} + {N_{\\cal{T}} \\over N_{\\cal{R}}} \\cal{I}_{\\cal{T}} ]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723775f-f231-45d0-9f0c-021095019121",
   "metadata": {},
   "source": [
    "# Splitting Criteria for Regression\n",
    "\n",
    "<center>\n",
    "    <img src=\"./img/Lecture-03/tree-split.png\", width = \"250\"/>   \n",
    "</center>\n",
    "\n",
    "Assume that the node $\\cal{R}$ has $N_{\\cal{R}}$ examples with targets $y_i, i = 1, \\ldots, N_{\\cal{R}}$.\n",
    "\n",
    "Prediction for node $\\cal{R}$: $\\bar{y}_{\\cal{R}} = {1 \\over N_{\\cal{R}}} \\sum_i^{N_{\\cal{R}}} y_i$ \n",
    "\n",
    "Impurity for Node $\\cal{R}$: \n",
    "\n",
    "1. Mean Squared Error: $\\cal{I}_{\\cal{R}} = {1 \\over N_{\\cal{R}}} \\sum_i^{N_{\\cal{R}}} (y_i - \\bar{y}_{\\cal{R}})^2$ \n",
    "\n",
    "\n",
    "\n",
    "Assume that $\\cal{R}$ is split into 2 nodes $\\cal{S}$ and $\\cal{T}$ with examples $N_{\\cal{R}}$ and $N_{\\cal{R}}$ respectively. \n",
    "\n",
    "\n",
    "Then the impurity loss from the split of $\\cal{R}$ into $\\cal{S}$ and $\\cal{T}$ is given by \n",
    "\n",
    "$ \\Delta \\cal{I} = \\cal{I}_{\\cal{R}} - \\left[ {N_{\\cal{S}} \\over N_{\\cal{R}}} \\cal{I}_{\\cal{S}} + {N_T \\over N_{\\cal{R}}} \\cal{I}_{\\cal{T}} \\right]$ \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93ea201-ce8d-4b19-ba39-52e7a92b8068",
   "metadata": {},
   "source": [
    "# Building a Decision Tree \n",
    "\n",
    "+ Splitting Candidates: Select the most informative features for splitting the data effectively.\n",
    "+ Splitting Criteria: Determine how to best split the data at each node (e.g., using entropy or information gain).\n",
    "+ Stopping Rules: Decide when to stop splitting the tree and assign a final prediction or category.\n",
    "+ Impurity Measures:\n",
    "    - Gini, Entropy for classification\n",
    "    - MSE for regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a13289-3e6b-4aa1-b180-b3fd9e5455f5",
   "metadata": {},
   "source": [
    "# Pruning a Decision Tree\n",
    "\n",
    "Grow a large tree $T_0$ and stop the splitting only when the node is either pure (all examples of the same class), or if the number of examples is smaller than some minimum node size.\n",
    "\n",
    "Let $T \\subset T_0$ be any tree that is obtained from T by collapsing the sub-tree at any of the internal nodes of $T_0$.  Let  $\\cal{R}_m$ denote the domain of the $m$'th terminal node of $T$, and let $|T|$ be the number of internal nodes of $T$  and let \n",
    "\n",
    "\\begin{align*}\n",
    "N_m &= \\mathrm{Number} ( x_i \\in R_m ), \\\\\n",
    "\\hat{y}_m &= {1 \\over N_m} \\sum_{y_i \\in R_m} y_i, \\\\\n",
    "\\cal{I}_m(T) &= {1 \\over N_m} \\sum_{y_i \\in R_m} (y_i - \\hat{y}_m).\n",
    "\\end{align*}\n",
    "\n",
    "Then the cost-complexity of the pruned tree $T$ is given by \n",
    "\n",
    "\\begin{align*}\n",
    "C_{\\lambda}(T) = \\sum_{m=1}^{|T|} N_m \\cal{I}_m(T) + \\lambda |T|.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fda36bf-3db0-43f4-871f-3780b7204e9b",
   "metadata": {},
   "source": [
    "# Pruning a Decision Tree\n",
    "\n",
    "\n",
    "<table>\n",
    "        <ul>\n",
    "        <font size = \"+1.5\">\n",
    "        <li> Why is pruning necessary?  The impurity loss can be driven to zero, e.g. if all the leaf nodes just have 1 example. </li>\n",
    "        <li> Such deep trees have very poor generalization (overfitting on the training data).</li>\n",
    "        <li>On the other hand, very shallow trees also do a very poor job of prediction (underfitting)</li>\n",
    "        <li>Hence some penalty is added to the Impurity function to penalize overly-deep trees.</li>\n",
    "        <li>The value of $\\lambda$ balances overfitting vs underfitting and is determined by cross validation.</li>\n",
    "        </font>\n",
    "        </ul>\n",
    "    </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f62062-1fcf-4b61-b836-387f979d18ef",
   "metadata": {},
   "source": [
    "# Classification Example\n",
    "\n",
    "Demonstration of multi-class classification using the Iris data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f100135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dd06471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 150 (50 in each of three classes)\n",
      ":Number of Attributes: 4 numeric, predictive attributes and the class\n",
      ":Attribute Information:\n",
      "    - sepal length in cm\n",
      "    - sepal width in cm\n",
      "    - petal length in cm\n",
      "    - petal width in cm\n",
      "    - class:\n",
      "            - Iris-Setosa\n",
      "            - Iris-Versicolour\n",
      "            - Iris-Virginica\n",
      "\n",
      ":Summary Statistics:\n",
      "\n",
      "============== ==== ==== ======= ===== ====================\n",
      "                Min  Max   Mean    SD   Class Correlation\n",
      "============== ==== ==== ======= ===== ====================\n",
      "sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "============== ==== ==== ======= ===== ====================\n",
      "\n",
      ":Missing Attribute Values: None\n",
      ":Class Distribution: 33.3% for each of 3 classes.\n",
      ":Creator: R.A. Fisher\n",
      ":Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      ":Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. dropdown:: References\n",
      "\n",
      "  - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "    Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "    Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "  - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "    (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "  - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "    Structure and Classification Rule for Recognition in Partially Exposed\n",
      "    Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "    Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "  - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "    on Information Theory, May 1972, 431-433.\n",
      "  - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "    conceptual clustering system finds 3 classes in the data.\n",
      "  - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "print(iris.DESCR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d48549-bd72-4a96-871c-855ce93f2654",
   "metadata": {},
   "source": [
    "We restrict the input features to \"petal length (cm)\" and \"petal width (cm)\" so that we can plot the 2-D example values and visualize the decision boundaries.  Of course, if you don't need visualizatons then all the input features can be used for modeling and prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51c955-db1e-48a5-9d9b-3ab2e8f456d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot  the training points\n",
    "\n",
    "feature_names = [\"petal length (cm)\", \"petal width (cm)\"]\n",
    "X_iris = iris.data[feature_names].values\n",
    "y_iris = iris.target\n",
    "\n",
    "p1 = plt.scatter(X_iris[:,0], X_iris[:, 1], c=y_iris, s=50, cmap=plt.cm.Paired)\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.legend(handles=p1.legend_elements()[0], labels=['Setosa', 'Versicolour', 'Virginica', 'Query'], loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8907fb-249e-4cd8-a5fd-f5f00db7a7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision tree classifier of depth 2 \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "# build the tree \n",
    "tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)\n",
    "tree_clf.fit(X_iris, y_iris)\n",
    "\n",
    "# visualize the model\n",
    "plot_tree(tree_clf, feature_names=feature_names, impurity=True)\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40397890-ae46-43f6-ba09-ac15830ddd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing the Tree Structure\n",
    "\n",
    "tree = tree_clf.tree_\n",
    "\n",
    "print(f\"node count: {tree.node_count}\")\n",
    "print(f\"max depth: {tree.max_depth}\")\n",
    "print(f\"max_n_classes: {tree.max_n_classes}\")\n",
    "print(f\"n_features: {tree.n_features}\")\n",
    "print(f\"n_outputs: {tree.n_outputs}\")\n",
    "print(f\"impurity:\\n {tree.impurity}\")\n",
    "print(f\"value:\\n {''.join(map(str, tree.value))}\")\n",
    "print(f\"n_node_samples:\\n {tree.n_node_samples}\")\n",
    "\n",
    "def depth(tree):\n",
    "    depth = np.zeros(tree.node_count)\n",
    "    stack = [(0, 0)]\n",
    "    while stack:\n",
    "        node, node_depth = stack.pop()\n",
    "        depth[node] = node_depth\n",
    "        if tree.children_left[node] != tree.children_right[node]:\n",
    "            stack.append((tree.children_left[node], node_depth + 1))\n",
    "            stack.append((tree.children_right[node], node_depth + 1))\n",
    "    return depth\n",
    "    \n",
    "print(f\"depth:\\n {depth(tree)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5f8039-0fc7-4a72-8049-eeaf220c0777",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "We use a synthetic dataset with 1 features and a continuous response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7063b7-00a1-4c0f-98a1-19eb07e0ac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def generateData(nsamp):\n",
    "    X_quad = np.random.rand(n_samp, 1) - 0.5  # a single random input feature\n",
    "    y_quad = X_quad ** 2 + 0.025 * np.random.randn(n_samp, 1)\n",
    "    return X_quad, y_quad\n",
    "\n",
    "    \n",
    "n_samp = 200\n",
    "X_quad, y_quad = generateData(n_samp)\n",
    "\n",
    "n_samp_test = 100\n",
    "X_quad_test, y_quad_test = generateData(n_samp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c5ae8-6161-4135-a49c-947fdd07e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression tree of dept 2\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2, random_state=42)\n",
    "tree_reg.fit(X_quad, y_quad)\n",
    "\n",
    "# visualize the model\n",
    "plot_tree(tree_reg, impurity=True)\n",
    "plt.show()\n",
    "\n",
    "# print the tree accuracy\n",
    "\n",
    "print('Test set mean squared error: %.5f'\n",
    "      % mean_squared_error(y_quad_test, tree_reg.predict(X_quad_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9774d24b-ace9-421d-a8e6-cdbfcae618c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions\n",
    "\n",
    "axis_range=[-0.5, 0.5, -0.05, 0.25]\n",
    "x1 = np.linspace(axis_range[0], axis_range[1], 500).reshape(-1, 1)\n",
    "y_pred = tree_reg.predict(x1)\n",
    "plt.axis(axis_range)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.plot(X_quad, y_quad, \"b.\")\n",
    "plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "\n",
    "th0, th1a, th1b = tree_reg.tree_.threshold[[0, 1, 4]]\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "plt.text(th0, 0.16, \"Depth=0\", fontsize=15)\n",
    "plt.text(th1a + 0.01, -0.01, \"Depth=1\", horizontalalignment=\"center\", fontsize=13)\n",
    "plt.text(th1b + 0.01, -0.01, \"Depth=1\", fontsize=13)\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=16)\n",
    "plt.title(\"max_depth=2\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4952fbd4-bec3-456b-871e-f3b99aabdd42",
   "metadata": {},
   "source": [
    "# Overfitting and Regularization\n",
    "\n",
    "We consider 3 trees with min_samples_leaf=1, 15, 30 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28861f-864a-437b-92a8-b9bb404638a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "min_samples_leaf = [1, 15, 30]\n",
    "tree_regs = [DecisionTreeRegressor(random_state=42, min_samples_leaf=msl).fit(X_quad, y_quad)  for msl in min_samples_leaf]\n",
    "x1 = np.linspace(-0.5, 0.5, 500).reshape(-1, 1)\n",
    "y_preds = [tree_reg.predict(x1) for tree_reg in tree_regs]\n",
    "\n",
    "fig, axes = plt.subplots(ncols=len(min_samples_leaf), figsize=(15, 4), sharey=True)\n",
    "\n",
    "for idx, (msl, tree_reg, y_pred) in enumerate(zip(min_samples_leaf, tree_regs, y_preds)):\n",
    "    plt.sca(axes[idx])\n",
    "    plt.plot(X_quad, y_quad, \"b.\")\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "    plt.axis([-0.5, 0.5, -0.05, 0.25])\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "    plt.title(f\"min_samples_leaf={msl}\")\n",
    "    \n",
    "    \n",
    "plt.show()\n",
    "\n",
    "\n",
    "for msl, tree_reg in zip(min_samples_leaf, tree_regs):\n",
    "    print(f\"min_samples_leaf = {msl}\" + \" \" +\n",
    "        f\"Test set mean squared error: {mean_squared_error(y_quad_test, tree_reg.predict(X_quad_test))}\")\n",
    "\n",
    "# plot the predictions    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7dace4-504c-430d-a33e-f79da880e38c",
   "metadata": {},
   "source": [
    "# Advantages of Decision Trees\n",
    "\n",
    "\n",
    "+ Ease of Intepretation due to axis parallel splits\n",
    "+ Handling of numerical and categorical features is straightforward\n",
    "+ Invariant to rescaling and normalization of the features\n",
    "+ Model tree can be transformed into simplified Rulesets which help with model interpretation and augment existing rule-based systems\n",
    "+ Used in complex decision making process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc560a0-f58c-42eb-8403-078a1929971e",
   "metadata": {},
   "source": [
    "# Issues with Decison Trees\n",
    "\n",
    "+ Greedy one-level approach to splitting is often sub-optimal\n",
    "\n",
    "+ Overfitting: Without pruning, prone to overfitting the training data, leading to poor performance on unseen data.\n",
    "\n",
    "+ High variance: Splitting rules are sensitive to slight changes in the training data, leading to uunstable predictions\n",
    "\n",
    "+ Curse of dimensionality: Performance can decline with increasing number of features in the data.\n",
    "\n",
    "+ Relatively inflexible and poor at modeling highly nonlinear structures (ex. Concentric rings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4ed48-41ef-40ed-9f3f-37744de9e406",
   "metadata": {},
   "source": [
    "#  Summary\n",
    "\n",
    "Decision trees are powerful and intuitive models for supervised learning, but with some limitations.  They are particularly susceptible to instability (high variance) or overfitting (high bias).  \n",
    "\n",
    "The basic decision tree is a building block for more sophisticated algorithms that will be studied later in the course such as **boosted trees** and **random forests**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
