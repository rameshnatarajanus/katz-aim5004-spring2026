{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9b5423d0-c8d6-4e32-9199-562c617421a2",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9b5423d0-c8d6-4e32-9199-562c617421a2"
      },
      "source": [
        "# Lecture 24: Deep Learning\n",
        "\n",
        "\n",
        "Applied Machine Learning\n",
        "\n",
        "\n",
        "Professor: __Ramesh Natarajan__ \\\n",
        "Email: **ramesh.natarajan@yu.edu** \\\n",
        "Department of Computer Science \\\n",
        "Stern College and Yeshiva University\n",
        "\n",
        "Date: Feb xx, 2024\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd242646-ea89-4668-a02f-01e61a1fb48d",
      "metadata": {
        "id": "fd242646-ea89-4668-a02f-01e61a1fb48d"
      },
      "source": [
        "## Acknowledgements\n",
        "\n",
        "1. The starting point for developing the notebook-based instruction material was the set of notebooks provided by the previous  course instructor__[Prof. Zach Glassman](https://www.linkedin.com/in/zachary-glassman-7088844b/)__.\n",
        "2. These notebooks have been augmented by similar instruction material taken from many, many other sources, including official documentation from many ML libraries and frameworks. I have adapted and modified the content where needed to make it compatible with student level, orientation and proficiency in this course.\n",
        "3. The primary addition sources include:\n",
        "   1. Course content from V. Kuleshov, Cornell Tech __[repo](https://github.com/kuleshov/cornell-cs5785-2020-applied-ml/blob/main/notebooks/lecture1-introduction.ipynb)__.\n",
        "   2. Book and support material from Hal Daume II, __[A course in Machine Learning](http://ciml.info)__. __[repo](https://github.com/hal3/ciml/)__\n",
        "   3. Book and support material from A. Geron, __[Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow (3rd edition)](https://homl.info/er3)__. __[repo](https://github.com/ageron/handson-ml3/blob/main/README.md)__\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13a71692-ee2d-43ee-84dc-99cd0fa42c4d",
      "metadata": {
        "id": "13a71692-ee2d-43ee-84dc-99cd0fa42c4d"
      },
      "source": [
        "# Announcements\n",
        "\n",
        "1. Problem Set 2 to be closed this week. Problem Set 3 still open for submission.\n",
        "2. Recitation will review Deep Learning.\n",
        "3. You should continue  to review and refresh yourself on the mathematical prerequisites for applied machine learning from part 1 for Deisenroth, Faisal and Ong, __[Mathematics for Machine Learning](https://mml-book.github.io/book/mml-book.pdf)__.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9082135e-4377-4bcc-aad2-ab5eaa046ce0",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9082135e-4377-4bcc-aad2-ab5eaa046ce0"
      },
      "source": [
        "\n",
        "# Deep Learning  \n",
        "\n",
        "\n",
        "<table>\n",
        "    <td width=\"40%\"><center>\n",
        "        <img src=\"https://github.com/rameshnatarajanus/katz-aim5004-fall2025/blob/main/notebooks/img/Lecture-24/deep-learning-creative.png?raw=1\" />      \n",
        "    </center>\n",
        "    Generated by Chat-GPT\n",
        "     </td>  \n",
        "    <td>\n",
        "<font size = \"+1.0\">\n",
        "Deep learning models are Multilayer Neural Networks which have led to especially significant improvements in image, video, speech and text modeling applications.  These improvements have been achieved through a combination of interesting algorithms (e.g. dropout regularization and stochastic gradient descent), rapid prototyping using several important frameworks that support automatic differentiation (e.g. Pytorch, Tensorflow, Jax),  high-performance hardware implementations on GPUs (for fast convolutions and matrix vector operations).  \n",
        "\n",
        "</td></table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22cee414",
      "metadata": {
        "id": "22cee414"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "954df281",
      "metadata": {
        "id": "954df281"
      },
      "source": [
        "# Training Neural Networks\n",
        "\n",
        "This notebook gives a brief introduction to training Neural Networks and some of the pitfalls and problems you will see along the way.\n",
        "\n",
        "It strives to give a basic overview of many topics, each one could easily be a lecture on its own."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12b826f",
      "metadata": {
        "id": "b12b826f"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Lets dive into a bit more detail on the gradient descent algorithm.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2aba33",
      "metadata": {
        "id": "5a2aba33"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    *make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ccdfa8",
      "metadata": {
        "id": "22ccdfa8"
      },
      "source": [
        "An *epoch* is one pass over the training data, and a back-propagation computation to adjust the model weights is carried out for each example in the training data.  \n",
        "\n",
        "For large datasets, this is too slow and inefficient, and therefore we consider using a batch of input examples for each back propagation step, using a  `batch_size` parameter .  In addition batching the data in this way is also very helpful for optimizing the use of parallel hardware such as GPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e20b1450",
      "metadata": {
        "id": "e20b1450"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "class TimingCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, times, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.times = times\n",
        "        self._start_time = None\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self._start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        self.times.append(time.time() - self._start_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821ebf8e",
      "metadata": {
        "id": "821ebf8e"
      },
      "outputs": [],
      "source": [
        "from tqdm.keras import TqdmCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec9ef25",
      "metadata": {
        "id": "fec9ef25"
      },
      "outputs": [],
      "source": [
        "def train(batch_size, epochs=200):\n",
        "    print(f'fitting for batch_size: {batch_size}')\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(2,)),\n",
        "        tf.keras.layers.Dense(64, activation='relu', kernel_initializer='he_uniform'),\n",
        "        tf.keras.layers.Dense(3)\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.SGD(learning_rate=0.005),\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    times = []\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        validation_data=(X_test, y_test),\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0,\n",
        "                        callbacks=[\n",
        "                            TimingCallback(times),\n",
        "                            TqdmCallback(verbose=1)\n",
        "                        ],\n",
        "                        )\n",
        "    return history, times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c86f1b2",
      "metadata": {
        "id": "9c86f1b2"
      },
      "outputs": [],
      "source": [
        "#batch_sizes = [1, 50, len(X_train)]\n",
        "batch_sizes = [16, 32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9362a99",
      "metadata": {
        "id": "f9362a99"
      },
      "outputs": [],
      "source": [
        "# note, this will take several minutes to run....\n",
        "results = [train(i) for i in batch_sizes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3ec6d66",
      "metadata": {
        "id": "a3ec6d66"
      },
      "outputs": [],
      "source": [
        "for (history, _), batch_size in zip(results, batch_sizes):\n",
        "    plt.plot(history.history['accuracy'], label=batch_size)\n",
        "plt.legend()\n",
        "plt.title('accuracy score')\n",
        "plt.xlabel('epoch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d64c9f2d",
      "metadata": {
        "id": "d64c9f2d"
      },
      "outputs": [],
      "source": [
        "for (_, times), batch_size in zip(results, batch_sizes):\n",
        "    plt.plot(times, '.-', label=batch_size)\n",
        "plt.legend()\n",
        "plt.title('times')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509ca8fe",
      "metadata": {
        "id": "509ca8fe"
      },
      "source": [
        "Lets talk about the three cases\n",
        "\n",
        "- `batch_size=1` : online gradient descent\n",
        "- `batch_size=N`: gradient descent\n",
        "- `bach_size> 1 AND < N`: minibatch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db27a2b",
      "metadata": {
        "id": "1db27a2b"
      },
      "source": [
        "### Stochastic gradient Descent\n",
        "\n",
        "- In this case we are doing a complete forward and back propagation through the network on each step\n",
        "- This is very slow\n",
        "- Introduces lots of randomness into the network (which is not entirely bad).\n",
        "- Only requires one sample at a time, so it can scale very well to big datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29648f97",
      "metadata": {
        "id": "29648f97"
      },
      "source": [
        "### Batch Gradient Descent\n",
        "\n",
        "- In this case we look at all samples in the training set and move due to their average gradient.\n",
        "- This can be faster as we need less steps in our optimization\n",
        "- Can be get caught in local minima easily\n",
        "- Requires all samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18c779fb",
      "metadata": {
        "id": "18c779fb"
      },
      "source": [
        "### Minibatch Gradient Descent\n",
        "\n",
        "- Combination of previous two\n",
        "- Batch size is not 1, but more like 32\n",
        "- Has advantages of being faster with fewer updates\n",
        "- Has enough randomness to not be as prone to local minima\n",
        "- Only requires a few samples, so can typically scale well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d387c60",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0d387c60"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Algorithms can also be sensitive to input feature scaling and transformations.  For example, lets look at if we can do better in our model by rescaling the pixel values.\n",
        "\n",
        "We will work with the fashion MNIST data from last lecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b027f53",
      "metadata": {
        "id": "4b027f53"
      },
      "outputs": [],
      "source": [
        "_data = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(train_images, train_labels), (test_images, test_labels) = _data\n",
        "\n",
        "def compile_and_fit(model):\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(train_images, train_labels,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    epochs=10,\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b24f903",
      "metadata": {
        "id": "8b24f903"
      },
      "source": [
        "Lets start with a simple model using no rescaling.  We will be able to get about 80% validation error (note for a better test we could also use a hold out set.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12fe7532",
      "metadata": {
        "id": "12fe7532"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import model_to_dot\n",
        "from IPython.display import Image, display\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(140, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "\n",
        "model_graph = model_to_dot(model, show_shapes=True, show_layer_names=True)\n",
        "display(Image(model_graph.create_png()))\n",
        "compile_and_fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739a1499",
      "metadata": {
        "id": "739a1499"
      },
      "source": [
        "Now we can add a `Rescaling` layer which will normalize all the inputs between 0 and 1 by dividing by the maximum value (255)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2799466a",
      "metadata": {
        "id": "2799466a"
      },
      "outputs": [],
      "source": [
        "train_images.max(), train_images.min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faafc743",
      "metadata": {
        "id": "faafc743"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Dense(140, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "compile_and_fit(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9553c5be",
      "metadata": {
        "id": "9553c5be"
      },
      "source": [
        "## Data Augmentation and Jittering\n",
        "\n",
        "We can see we do much better.  We can also introduce other augmentation techniques like shifting the data, rotating the data.  Here we will randomly rotate the image by a little bit.  \n",
        "\n",
        "*Note:* These layers will be active only during training, but not during prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8a86c3",
      "metadata": {
        "id": "6f8a86c3"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.RandomRotation(0.05),\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(140, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    epochs=10,\n",
        "                    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9213cd4c",
      "metadata": {
        "id": "9213cd4c"
      },
      "source": [
        "The augmentation is useful because it helps our model generalize to more possibilities.  It can even make the network resilient to things like rotation which often crop up in real world problems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86780841",
      "metadata": {
        "id": "86780841"
      },
      "source": [
        "# Vanishing gradients\n",
        "\n",
        "We know that the training requires understanding the computation of the  gradient using back propagation (via the chain rule of differentiation), which requires taking the product of the derivatives of the activation function that is used in each hidden layer.\n",
        "\n",
        "As can be seen from the figure below for the case where the activation function is the hyperbolic tangent, the derivative of this function is zero except for small region of activation (between $\\pm 2$).\n",
        "\n",
        "This is known as the **vanishing gradient problem** since the network weights will not be updated in this case.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3666327c",
      "metadata": {
        "id": "3666327c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.linspace(-5, 5, 100)\n",
        "plt.figure(figsize=(4,3))\n",
        "plt.plot(x, np.tanh(x), label='tanh')\n",
        "plt.plot(x, 1/(np.cosh(x)**2), label='d/dx(tanh)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3610535e",
      "metadata": {
        "id": "3610535e"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "In order to avoid the vanishing gradients problem, the activation function have been proposed\n",
        "\n",
        "\n",
        "- ReLU Rectified Linear Unit\n",
        "- Leaky ReLU\n",
        "- Exponential Linear Unit\n",
        "- Scaled Exponential Linear Unit\n",
        "\n",
        "The ReLU is a very common choice since it avoids the vanishing gradients problems, since the gradient (at least for positive inputs) is always 1.\n",
        "\n",
        "While it helps induce sparsity in the network due to its one-sidedness, in some cases this can also lead to segments of the neural network having vanishing gradient.  An alternative in this case is the  Leaky RelU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9924761b",
      "metadata": {
        "id": "9924761b"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras import activations\n",
        "\n",
        "x = np.linspace(-3, 3, 100)\n",
        "plt.figure(figsize=(4,3))\n",
        "for func in ['relu',  'selu', 'elu', 'leaky_relu']:\n",
        "    plt.plot(x, getattr(tf.keras.activations, func)(x), label=func)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8d39fc-d881-428f-8eae-4b29ef2053c5",
      "metadata": {
        "id": "fa8d39fc-d881-428f-8eae-4b29ef2053c5"
      },
      "source": [
        "## Exploding gradients\n",
        "\n",
        "Exploding gradient problem - While we have solved the vanishing gradient problem, sometimes large unbounded gradients can cause the gradient to get too large and blow up.  Often can be solved by proper initialization.\n",
        "\n",
        "Takeaways - Lots of intricacies to training, need to keep track of proper initialization, activation functions etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ab0826",
      "metadata": {
        "id": "84ab0826"
      },
      "source": [
        "# Optimizers\n",
        "\n",
        "Many optimizers have been developed for Neural Networks that address some of the issues with the use of gradient descent.\n",
        "\n",
        "For example, in SGD we have a `step_size` hyperparameter which controls the learning rate.  However this quantity can be hard to set without some experimentation - too large and it is hard to converge, too small and it converges but very slowly.\n",
        "\n",
        "\n",
        "Lets look at a simple case with a few different learning rates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "753677ca",
      "metadata": {
        "id": "753677ca"
      },
      "outputs": [],
      "source": [
        "def loss(x):\n",
        "    return .5 * x ** 2\n",
        "def optimize(start_value, step_size, steps=20):\n",
        "    vals = [start_value]\n",
        "    for step in range(steps):\n",
        "        val = vals[-1]\n",
        "        vals.append(val - step_size * val)\n",
        "    return np.array(vals)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1)\n",
        "x = np.linspace(-5, 5)\n",
        "ax[0].plot(x, loss(x))\n",
        "colors = ['r', 'g', 'orange']\n",
        "for step_size, c in zip([.01, .5, 1.7], colors):\n",
        "    opt = optimize(5, step_size)\n",
        "    ax[0].plot(opt, loss(opt), '.-', label=f'step_size={step_size}', c=c)\n",
        "    ax[1].plot(opt, '.-', c=c)\n",
        "ax[0].legend()\n",
        "ax[1].set_xlabel('step')\n",
        "ax[1].set_ylabel('val')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0b4a2e2",
      "metadata": {
        "id": "c0b4a2e2"
      },
      "source": [
        "In order to improve this, there are a number of methods which help by various methods such as\n",
        "\n",
        "momentum - (automatic) modify  gradients based on the values of previous gradients\n",
        "adaptive learning rates - (user specified) manually control learning rates based on so-called learning rate curves\n",
        "\n",
        "We can see how some of these work in practice by using some of the Tensorflow optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08380bde",
      "metadata": {
        "id": "08380bde"
      },
      "outputs": [],
      "source": [
        "def run(optimizer, start_value, steps=100):\n",
        "    var = tf.Variable(start_value)\n",
        "    loss = lambda: .5 * var**2\n",
        "\n",
        "    vals = [start_value]\n",
        "    for step in range(steps):\n",
        "        with tf.GradientTape() as g:\n",
        "            g.watch(var)\n",
        "            t = loss()\n",
        "            gradients = g.gradient(t, [var])\n",
        "        optimizer.apply_gradients(zip(gradients, [var]))\n",
        "        vals.append(var.numpy())\n",
        "    return vals\n",
        "  # Calculate the gradients\n",
        "\n",
        "LR = .05\n",
        "for opt in [\n",
        "    tf.keras.optimizers.SGD(learning_rate=LR),\n",
        "    tf.keras.optimizers.RMSprop(learning_rate=LR),\n",
        "    tf.keras.optimizers.Adam(learning_rate=LR)\n",
        "]:\n",
        "    vals = run(opt, 2.0)\n",
        "    plt.plot(vals, '.-', label=opt.__class__.__name__)\n",
        "plt.legend()\n",
        "plt.xlabel('step')\n",
        "plt.ylabel('var')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3c859a0",
      "metadata": {
        "id": "d3c859a0"
      },
      "source": [
        "# Dropout\n",
        "\n",
        "We have many parameters in deep learning models,  If you ever want to check, look at the model summary (need to compile it first)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d6f8c9",
      "metadata": {
        "id": "95d6f8c9"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Rescaling(1./255),\n",
        "    tf.keras.layers.Dense(140, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e7e69ae",
      "metadata": {
        "id": "7e7e69ae"
      },
      "source": [
        "Even simple networks will have hundreds of thousands of parameters, so how to avoid overfitting?\n",
        "\n",
        "The simplest method, and on that is perhaps surprisingly successful is the notion of dropout (although hopefully it won't be so surprising why after this lecture :))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "838e7079",
      "metadata": {
        "id": "838e7079"
      },
      "source": [
        "The basic idea in dropout is the following\n",
        "\n",
        "1. On each sample, randomly drop each weight with probability $p$\n",
        "2. Rescale all weights to account for dropped weights\n",
        "3. Proceed as normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3534c6f",
      "metadata": {
        "id": "e3534c6f"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(250, activation='relu'),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(train_images, train_labels,\n",
        "            validation_data=(test_images, test_labels),\n",
        "            epochs=100,\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(patience=3)]\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b4e265",
      "metadata": {
        "id": "d0b4e265"
      },
      "source": [
        "### Why does this work?\n",
        "\n",
        "A smaller network is less prone to overfitting\n",
        "\n",
        "Each individual iteration is training a smaller network (kind of a like a Random Forest!).\n",
        "\n",
        "Another way to think about it, is that each individual neuron cannot become too specialized as its only there part of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a07094f",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4a07094f"
      },
      "source": [
        "# Checkpointing and Persisting Models\n",
        "\n",
        "Once we train models, we need to persist them so that we can load them later to make predictions.  To do this is actually relatively simple as long as your entire \"pipeline\" is in a single model.\n",
        "\n",
        "**Note:** not working in JupyterHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8e4826",
      "metadata": {
        "id": "7e8e4826"
      },
      "outputs": [],
      "source": [
        "model.save('./saved_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "995e7252",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "995e7252"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "list(Path('./saved_model.keras').glob(\"*\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "092a4216",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "092a4216"
      },
      "outputs": [],
      "source": [
        "list(Path('./saved_model.keras/variables').glob(\"*\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7f69ab",
      "metadata": {
        "id": "8b7f69ab"
      },
      "source": [
        "Tensorflow is saving a few things:\n",
        "\n",
        "1. The model architecture (in the .pb file)\n",
        "2. The weights - saved in the variables directory\n",
        "\n",
        "We can load it as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81532db1",
      "metadata": {
        "id": "81532db1"
      },
      "outputs": [],
      "source": [
        "saved_model = tf.keras.models.load_model('./saved_model.keras')\n",
        "saved_model.layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "203682a9",
      "metadata": {
        "id": "203682a9"
      },
      "source": [
        "Often we might train a model on one machine which has lots of compute and then run inference on a different machine, say a web service.  This persistence allows us to do this quite easily."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af870a52",
      "metadata": {
        "id": "af870a52"
      },
      "source": [
        "# Transfer Learning\n",
        "\n",
        "One of the most important innovations in ML (at least in a practical sense) is the idea of Transfer Learning\n",
        "\n",
        "Transfer learning allows us to train models on a large amount of general data and then reuse those models to train specialized models.\n",
        "\n",
        "Intuitively, we can think of these pre-trained models as really awesome feature extractors which we can then use to train smaller neural networks (or tune them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb95134",
      "metadata": {
        "id": "dfb95134"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85033db",
      "metadata": {
        "id": "e85033db"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(train_images[25])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b34f2ac",
      "metadata": {
        "id": "4b34f2ac"
      },
      "source": [
        "We are just focusing on the training here, however, it would be a good idea to leave out some of the dataset for final testing to make sure that as we tune hyperparameters we are not including any knowledge we shouldn't be including.\n",
        "\n",
        "Lets start by just training our previous model on this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021af26a",
      "metadata": {
        "id": "021af26a"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.Rescaling(1./255),\n",
        "        tf.keras.layers.Dense(140, activation='relu'),\n",
        "        tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    validation_data=(test_images, test_labels), epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ead9c2",
      "metadata": {
        "id": "73ead9c2"
      },
      "source": [
        "We see this model didn't do so well.  The reason is that this model does not have the requisite complexity to capture the details of this dataset.\n",
        "\n",
        "On the other hand, this dataset is not super large, can we train a complex model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33f94246",
      "metadata": {
        "id": "33f94246"
      },
      "source": [
        "Luckily we don't have to.  We can use a pre-trained model which is trained on large amounts of relatively similiar data, then simply train a small model on top to encode our particular problem.\n",
        "\n",
        "The intuitive idea is that these large models are learning lots of fundamental features about images, we can use them as feature extractors.\n",
        "\n",
        "Keras makes this quite easy, it comes with a bunch of pre-built 'applications' for things like image process as well as NLP.  We will use the `MobileNetV3Small` for demonstration purposes here.\n",
        "\n",
        "First we need to create the model, notice the model is set as `trainable=False`, this is because we do not at this point want to train the weights of this model, only use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be5de1e4",
      "metadata": {
        "id": "be5de1e4"
      },
      "outputs": [],
      "source": [
        "def get_base_model():\n",
        "    base_model = tf.keras.applications.MobileNetV3Small(\n",
        "        input_shape=(224, 224, 3),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "    return base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c8449c",
      "metadata": {
        "id": "a1c8449c"
      },
      "source": [
        "Now we can create a function `get_model` which will create our overall model.  We will use an alternate way of create models in Keras, sometimes called the *functional* API.  It will make it a bit easier to use our pre-trained model.  Note the following\n",
        "\n",
        "- We rescale the images, models often will have an expected feature input.\n",
        "- We apply a `GlobalAveragePooling2D` layer.  This averages over the feature vector so we can then apply our final classification layers to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9dac35",
      "metadata": {
        "id": "fd9dac35"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "    base_model = get_base_model()\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    scale =  tf.keras.layers.Lambda(\n",
        "        lambda image : tf.image.resize_with_pad(image, 224, 224)\n",
        "    )(inputs)\n",
        "    rescale = tf.keras.applications.mobilenet_v3.preprocess_input(scale)\n",
        "    x = base_model(rescale, training=False)\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = tf.keras.layers.Dropout(0.2)(x)\n",
        "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(.3)(x)\n",
        "    outputs = tf.keras.layers.Dense(10)(x)\n",
        "    return tf.keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e975ee5-c540-449d-9482-799928e66766",
      "metadata": {
        "id": "7e975ee5-c540-449d-9482-799928e66766"
      },
      "outputs": [],
      "source": [
        "get_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400ab2ea",
      "metadata": {
        "id": "400ab2ea"
      },
      "source": [
        "We will use a strategy here.  Typically we do not need to do this, however, if we want to make use of a TPU or GPU, this will make the code translate more easily.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bce0a270",
      "metadata": {
        "id": "bce0a270"
      },
      "outputs": [],
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "with strategy.scope():\n",
        "    model = get_model()\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "875433a3",
      "metadata": {
        "id": "875433a3"
      },
      "source": [
        "Note that this next cell will run really slowly if we only have CPU installed, so we will only run for a little bit, however, we can switch to a GPU or TPU with almost the same code and see a very large performance gain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd639f0",
      "metadata": {
        "id": "1cd639f0"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_images,\n",
        "    train_labels,\n",
        "    validation_data=(test_images, test_labels),\n",
        "    epochs=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5b0a2f",
      "metadata": {
        "id": "dd5b0a2f"
      },
      "source": [
        "If we want to squeeze a bit more performance out, we can actually retrain part of the original network, however, we want to make sure we are not retraining from start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31eb3fc6",
      "metadata": {
        "id": "31eb3fc6"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab750af4",
      "metadata": {
        "id": "ab750af4"
      },
      "outputs": [],
      "source": [
        "base_model = model.layers[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "186c8a97",
      "metadata": {
        "id": "186c8a97"
      },
      "outputs": [],
      "source": [
        "base_model.trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "670693c2",
      "metadata": {
        "id": "670693c2"
      },
      "source": [
        "We will make the base model trainable, but only the final few layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cece765e",
      "metadata": {
        "id": "cece765e"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:200]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4833a9c",
      "metadata": {
        "id": "d4833a9c"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  optimizer = tf.keras.optimizers.RMSprop(learning_rate=.00001),\n",
        "                  metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f574a5b",
      "metadata": {
        "id": "6f574a5b"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399b7c66",
      "metadata": {
        "id": "399b7c66"
      },
      "source": [
        "Now we can continue training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vmLlrVwgbLCI"
      },
      "id": "vmLlrVwgbLCI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d0be971",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "2d0be971"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "    history_final = model.fit(\n",
        "        train_images,\n",
        "        train_labels,\n",
        "        validation_data=(test_images, test_labels),\n",
        "        initial_epoch=history.epoch[-1],\n",
        "        epochs=2\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e435cbf",
      "metadata": {
        "id": "2e435cbf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}